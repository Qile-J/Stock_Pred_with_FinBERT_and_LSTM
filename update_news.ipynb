{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update recent news headlines and perform sentiment analysis with FinBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping of news headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching data for week 1 starting from 2023-07-13\n",
      "Finished fetching data for week 2 starting from 2023-07-27\n",
      "Finished fetching data for week 3 starting from 2023-08-10\n",
      "Finished fetching data for week 4 starting from 2023-08-24\n",
      "Finished fetching data for week 5 starting from 2023-09-07\n",
      "Finished fetching data for week 6 starting from 2023-09-21\n",
      "Finished fetching data for week 7 starting from 2023-10-05\n",
      "Finished fetching data for week 8 starting from 2023-10-19\n",
      "Finished fetching data for week 9 starting from 2023-11-02\n",
      "Finished fetching data for week 10 starting from 2023-11-16\n",
      "Finished fetching data for week 11 starting from 2023-11-30\n",
      "Finished fetching data for week 12 starting from 2023-12-14\n",
      "Finished fetching data for week 13 starting from 2023-12-28\n",
      "Finished fetching data for week 14 starting from 2024-01-11\n",
      "Finished fetching data for week 15 starting from 2024-01-25\n",
      "Finished fetching data for week 16 starting from 2024-02-08\n",
      "Finished fetching data for week 17 starting from 2024-02-22\n",
      "Finished fetching data for week 18 starting from 2024-03-07\n",
      "Finished fetching data for week 19 starting from 2024-03-21\n",
      "Finished fetching data for week 20 starting from 2024-04-04\n",
      "Finished fetching data for week 21 starting from 2024-04-18\n",
      "Finished fetching data for week 22 starting from 2024-05-02\n",
      "Finished fetching data for week 23 starting from 2024-05-16\n",
      "Finished fetching data for week 24 starting from 2024-05-30\n",
      "Finished fetching data for week 25 starting from 2024-06-13\n",
      "Finished fetching data for week 26 starting from 2024-06-27\n",
      "Finished fetching data for week 27 starting from 2024-07-11\n",
      "News headlines saved to AAPL_NEWS_2024-07-13.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import finnhub\n",
    "\n",
    "api_key = 'your_api_key'\n",
    "finnhub_client = finnhub.Client(api_key=api_key)\n",
    "\n",
    "def fetch_news_articles(symbol, start_date, end_date):\n",
    "    try:\n",
    "        news = finnhub_client.company_news(symbol, _from=start_date, to=end_date)\n",
    "        return news\n",
    "    except Exception:  # Catch any exception\n",
    "        print(f\"Error fetching news for {start_date} to {end_date}. Skipping...\")\n",
    "        return []\n",
    "\n",
    "symbol = 'AAPL'\n",
    "start_date = datetime.today() - timedelta(days=366)\n",
    "end_date = datetime.today()\n",
    "delta = timedelta(days=2)\n",
    "\n",
    "# Loop to fetch news articles in delta-day intervals\n",
    "current_start_date = start_date\n",
    "week_counter = 0\n",
    "all_news = []\n",
    "\n",
    "while current_start_date < end_date:\n",
    "    current_end_date = current_start_date + delta\n",
    "    if current_end_date > end_date:\n",
    "        current_end_date = end_date\n",
    "    \n",
    "    news_articles = fetch_news_articles(symbol, current_start_date.strftime('%Y-%m-%d'), current_end_date.strftime('%Y-%m-%d'))\n",
    "    all_news.extend(news_articles)\n",
    "    \n",
    "    if (current_start_date - start_date).days % 7 == 0:\n",
    "        week_counter += 1\n",
    "        print(f\"Finished fetching data for week {week_counter} starting from {current_start_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    current_start_date = current_end_date\n",
    "    time.sleep(1)  # To avoid exceeding the API rate limit\n",
    "\n",
    "# Convert the news data to a DataFrame\n",
    "news_df = pd.DataFrame(all_news)\n",
    "news_df['datetime'] = news_df['datetime'].apply(lambda x: datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "news_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save File\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "filename = f\"{symbol}_NEWS_{today_date}.csv\"\n",
    "news_df.to_csv(filename, index=False)\n",
    "print(f\"News headlines saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17191/17191 [07:21<00:00, 38.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment summary saved to AAPL_sentiment_2024-07-13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if has_mps else \"cuda\" if has_gpu else \"cpu\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")\n",
    "\n",
    "# Load the saved news data\n",
    "sym = 'AAPL'\n",
    "date = '2024-07-13'\n",
    "file_path = f'../data/{sym}_NEWS_{date}.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", num_labels=3).to(device)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
    "    return e_x / np.sum(e_x, axis=1)[:, None]\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    probs = softmax(logits)\n",
    "    probs = np.squeeze(probs)   # [positive, negative, neutral]\n",
    "    return probs[0] - probs[1]  # positive - negative\n",
    "\n",
    "# Apply sentiment analysis with progress bar\n",
    "tqdm.pandas()\n",
    "df['sentiment'] = df['headline'].progress_apply(get_sentiment_score)\n",
    "\n",
    "# Compute the average sentiment score and headline count for each date\n",
    "summary = df.groupby('datetime').agg(\n",
    "    avg_sentiment=('sentiment', 'mean'),\n",
    "    count=('headline', 'count')\n",
    ").reset_index().sort_values(by='datetime')\n",
    "\n",
    "# Save the sentiment summary to a CSV file\n",
    "summary_file = f\"{sym}_sentiment_{date}.csv\"\n",
    "summary.to_csv(summary_file, index=False)\n",
    "print(f\"Sentiment summary saved to {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing dates in sentiment data: DatetimeIndex(['2023-07-13', '2023-07-14', '2023-07-15', '2023-07-16',\n",
      "               '2023-07-17', '2023-07-18', '2024-07-13'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.today() - timedelta(days=366)\n",
    "end_date = datetime.today()\n",
    "date_range = pd.date_range(start=start_date, end=end_date).strftime('%Y-%m-%d')\n",
    "\n",
    "sym = 'AAPL'\n",
    "sentiment_date = '2024-07-13'\n",
    "sentiment_file_path = f'../data/{sym}_sentiment_{sentiment_date}.csv'\n",
    "sentiment_df = pd.read_csv(sentiment_file_path)\n",
    "\n",
    "# Check for missing dates in the sentiment data\n",
    "missing_dates = pd.to_datetime(date_range).difference(pd.to_datetime(sentiment_df['datetime']))\n",
    "print(\"Missing dates in sentiment data:\", missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Sentiment and Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to AAPL_Merge_2024-07-10.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define symbol and dates\n",
    "sym = 'AAPL'\n",
    "stock_date = '2024-07-11'\n",
    "sentiment_date = '2024-07-13'\n",
    "\n",
    "# Load the historical stock price data\n",
    "stock_file_path = f'../data/{sym}_{stock_date}.csv'\n",
    "stock_df = pd.read_csv(stock_file_path)\n",
    "\n",
    "# Load the sentiment summary data\n",
    "sentiment_file_path = f'../data/{sym}_sentiment_{sentiment_date}.csv'\n",
    "sentiment_df = pd.read_csv(sentiment_file_path)\n",
    "\n",
    "# Merge the two dataframes by date\n",
    "merged_df = pd.merge(stock_df, sentiment_df, left_on='Date', right_on='datetime')\n",
    "\n",
    "# Drop the redundant 'datetime' column from the merged dataframe\n",
    "merged_df.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a new file\n",
    "last_date = merged_df['Date'].max()\n",
    "merge_file_path = f'{sym}_Merge_{last_date}.csv'\n",
    "merged_df.to_csv(merge_file_path, index=False)\n",
    "\n",
    "print(f\"Merged data saved to {merge_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
